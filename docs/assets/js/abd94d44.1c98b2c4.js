"use strict";(self.webpackChunkomni_context_cli_landing=self.webpackChunkomni_context_cli_landing||[]).push([[675],{7833(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"cost-saving","title":"Cost Saving Guide","description":"Running large language models can get expensive, especially for extended conversations and complex tasks. This guide covers practical techniques to reduce costs while maintaining quality.","source":"@site/markdown/cost-saving.md","sourceDirName":".","slug":"/tutorial/cost-saving","permalink":"/omni-context-cli-landing/docs/tutorial/cost-saving","draft":false,"unlisted":false,"editUrl":"https://github.com/bluenoah1991/omni-context-cli-landing/tree/main/markdown/cost-saving.md","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"slug":"/tutorial/cost-saving","title":"Cost Saving Guide","sidebar_label":"Cost Saving Guide","sidebar_position":12},"sidebar":"defaultSidebar","previous":{"title":"Session Management","permalink":"/omni-context-cli-landing/docs/tutorial/sessions"}}');var t=i(4848),o=i(8453);const r={slug:"/tutorial/cost-saving",title:"Cost Saving Guide",sidebar_label:"Cost Saving Guide",sidebar_position:12},l="Cost Saving Guide",c={},a=[{value:"Understanding Cost Factors",id:"understanding-cost-factors",level:2},{value:"Leverage Specialist Mode",id:"leverage-specialist-mode",level:2},{value:"How Specialist Mode Saves Money",id:"how-specialist-mode-saves-money",level:3},{value:"Recommended Configuration",id:"recommended-configuration",level:3},{value:"The Key to Effective Model Pairing",id:"the-key-to-effective-model-pairing",level:3},{value:"Optimize Cache TTL",id:"optimize-cache-ttl",level:2},{value:"Why 1-Hour Cache Works Better",id:"why-1-hour-cache-works-better",level:3},{value:"Trim Thinking Blocks",id:"trim-thinking-blocks",level:2},{value:"The Impact",id:"the-impact",level:3},{value:"Optimized System Design",id:"optimized-system-design",level:2},{value:"Summary of Best Practices",id:"summary-of-best-practices",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"cost-saving-guide",children:"Cost Saving Guide"})}),"\n",(0,t.jsx)(n.p,{children:"Running large language models can get expensive, especially for extended conversations and complex tasks. This guide covers practical techniques to reduce costs while maintaining quality."}),"\n",(0,t.jsx)(n.h2,{id:"understanding-cost-factors",children:"Understanding Cost Factors"}),"\n",(0,t.jsx)(n.p,{children:"LLM costs are primarily driven by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Request frequency"}),": Every API call resends your conversation history"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context size"}),": Larger contexts consume more tokens"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cache hits"}),": Proper caching reduces repeated token processing"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"leverage-specialist-mode",children:"Leverage Specialist Mode"}),"\n",(0,t.jsxs)(n.p,{children:["Specialist mode is one of the most effective cost-saving features in Omx. It uses ",(0,t.jsx)(n.strong,{children:"Agentic tools"})," instead of basic tools to optimize both request count and context quality."]}),"\n",(0,t.jsx)(n.h3,{id:"how-specialist-mode-saves-money",children:"How Specialist Mode Saves Money"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Reduced Request Rounds"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Basic tools often require multiple sequential calls to complete a task"}),"\n",(0,t.jsx)(n.li,{children:"Agentic tools aggregate multiple operations into a single call"}),"\n",(0,t.jsx)(n.li,{children:"Fewer requests mean fewer full-context transmissions"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Less Context Noise"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Basic tools return intermediate outputs that clutter your conversation"}),"\n",(0,t.jsx)(n.li,{children:"Agentic tools provide only the final, relevant results"}),"\n",(0,t.jsx)(n.li,{children:"Cleaner context means more efficient token usage"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"recommended-configuration",children:"Recommended Configuration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Main Model: Opus 4.5 (or similar high-end model)\nAgent Model: GLM-4.7 (or similar cost-effective model)\n"})}),"\n",(0,t.jsx)(n.p,{children:"This combination gives you the best of both worlds: powerful reasoning for complex tasks with economical execution of routine operations."}),"\n",(0,t.jsx)(n.h3,{id:"the-key-to-effective-model-pairing",children:"The Key to Effective Model Pairing"}),"\n",(0,t.jsx)(n.p,{children:"Combining high-end models with cost-effective models is a common cost optimization strategy. However, pairing a 90-point model with a 60-point model poorly can yield results below 80 points."}),"\n",(0,t.jsx)(n.p,{children:'The critical factor is to prevent the cost-effective model from participating in decision-making. Instead, restrict it to handling the "dirty work" \u2014 routine execution tasks that don\'t require advanced reasoning.'}),"\n",(0,t.jsx)(n.p,{children:"Agentic tools represent the optimal division of labor, refined through extensive experimentation. They ensure that your powerful model focuses on high-level strategy and decision-making while delegating repetitive operations to the economical model."}),"\n",(0,t.jsx)(n.h2,{id:"optimize-cache-ttl",children:"Optimize Cache TTL"}),"\n",(0,t.jsx)(n.p,{children:"When working on complex problems that require extended thinking time, adjust your cache TTL setting:"}),"\n",(0,t.jsx)(n.h3,{id:"why-1-hour-cache-works-better",children:"Why 1-Hour Cache Works Better"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"5-minute cache"})," (default): Expires quickly if you pause to think or research"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"1-hour cache"}),": Stays valid throughout your problem-solving session"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"While 1-hour caches have higher creation costs, they eliminate repeated cache rebuilding during longer conversations. For complex tasks, this is significantly more cost-effective."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"When to use 1-hour cache:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Debugging complex issues"}),"\n",(0,t.jsx)(n.li,{children:"Multi-step refactoring"}),"\n",(0,t.jsx)(n.li,{children:"Research and analysis tasks"}),"\n",(0,t.jsx)(n.li,{children:"Any work requiring extended focus"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"trim-thinking-blocks",children:"Trim Thinking Blocks"}),"\n",(0,t.jsx)(n.p,{children:"Advanced models like Opus 4.5 include detailed thinking blocks that show their reasoning process. While valuable for transparency, they significantly increase context size."}),"\n",(0,t.jsx)(n.h3,{id:"the-impact",children:"The Impact"}),"\n",(0,t.jsx)(n.p,{children:"Thinking blocks can consume 30-50% of your context in extended conversations. Trimming them:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Reduces token consumption per request"}),"\n",(0,t.jsx)(n.li,{children:"Keeps your conversation focused on results"}),"\n",(0,t.jsx)(n.li,{children:"Maintains context for more messages before compression is needed"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Omx automatically handles thinking block optimization so you get the benefits without manual management."}),"\n",(0,t.jsx)(n.h2,{id:"optimized-system-design",children:"Optimized System Design"}),"\n",(0,t.jsx)(n.p,{children:"OmniContext CLI is engineered for efficiency:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Streamlined system prompts"}),": Minimal, focused instructions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Concise tool descriptions"}),": Only essential information"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Efficient context management"}),": Smart compression and editing"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This design philosophy ensures that your tokens are used for your actual work, not bloated framework overhead."}),"\n",(0,t.jsx)(n.h2,{id:"summary-of-best-practices",children:"Summary of Best Practices"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Enable specialist mode"})," for complex tasks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use high-end main models"})," with cost-effective agent models"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Set cache to 1-hour"})," for extended problem-solving sessions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Let Omx trim thinking blocks"})," automatically"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"These techniques work together to maximize value while minimizing unnecessary expenses."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>l});var s=i(6540);const t={},o=s.createContext(t);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);