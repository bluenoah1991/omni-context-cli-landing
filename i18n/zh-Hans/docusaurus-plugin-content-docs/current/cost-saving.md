---
slug: /tutorial/cost-saving
title: 大模型省钱指南
sidebar_label: 大模型省钱指南
sidebar_position: 12
---

# 大模型省钱指南

运行大语言模型可能会很昂贵，特别是对于长时间对话和复杂任务。本指南介绍实用的技巧来在保持质量的同时降低成本。

## 理解成本因素

大模型的成本主要受以下因素影响：

- **请求频率**：每次 API 调用都会重新发送你的对话历史
- **上下文大小**：更大的上下文消耗更多 token
- **缓存命中**：适当的缓存可以减少重复的 token 处理

## 利用专家模式

专家模式是 Omx 中最有效的省钱功能之一。它使用 **Agentic 工具** 而不是基础工具来优化请求次数和上下文质量。

### 专家模式如何省钱

**减少请求轮次**

- 基础工具通常需要多次顺序调用来完成任务
- Agentic 工具将多个操作聚合到单次调用中
- 更少的请求意味着更少的完整上下文传输

**减少上下文噪音**

- 基础工具返回的中间输出会污染你的对话
- Agentic 工具只提供最终的、相关的结果
- 更简洁的上下文意味着更高效的 token 使用

### 推荐配置

```
主模型：Opus 4.5（或类似的高端模型）
Agent 模型：GLM-4.7（或类似的性价比模型）
```

这种组合让你两全其美：复杂任务的强大推理能力，以及常规操作的经济执行。

### 有效模型配对的关键

将强力模型与性价比模型结合是常见的成本优化策略。但是，如果搭配不当，90 分的模型和 60 分的模型组合，结果往往低于 80 分。

关键在于不要让性价比模型参与决策，而是只让它专注于"脏活累活"——那些不需要高级推理的重复性执行任务。

Agentic 工具代表了经过反复实验后得出的最佳分工方案。它们确保你的强力模型专注于高层战略和决策制定，同时将重复性操作委托给经济型模型。

## 优化缓存 TTL

在处理需要长时间思考的复杂问题时，调整你的缓存 TTL 设置：

### 为什么 1 小时缓存更好

- **5 分钟缓存**（默认）：如果你暂停思考或研究，会很快过期
- **1 小时缓存**：在整个问题解决会话期间保持有效

虽然 1 小时缓存的创建成本更高，但它们消除了长时间对话中的重复缓存重建。对于复杂任务，这明显更具成本效益。

**何时使用 1 小时缓存：**
- 调试复杂问题
- 多步骤重构
- 研究和分析任务
- 任何需要长时间专注的工作

## 裁剪思考块

Opus 4.5 等高级模型包含详细的思考块，展示其推理过程。虽然对透明度有价值，但它们显著增加了上下文大小。

### 影响

在长时间的对话中，思考块可能消耗上下文的 30-50%。裁剪它们可以：

- 减少每次请求的 token 消耗
- 保持对话专注于结果
- 在需要压缩之前为更多消息保持上下文

Omx 自动处理思考块优化，你可以获得好处而无需手动管理。

## 优化的系统设计

OmniContext CLI 专为效率而设计：

- **精简的系统提示词**：最少、专注的指令
- **简洁的工具描述**：仅包含必要信息
- **高效的上下文管理**：智能压缩和编辑

这种设计理念确保你的 token 用于实际工作，而不是臃肿的框架开销。

## 最佳实践总结

1. **启用专家模式** 用于复杂任务
2. **使用高端主模型** 搭配性价比 Agent 模型
3. **将缓存设置为 1 小时** 用于长时间的问题解决会话
4. **让 Omx 自动裁剪思考块**

这些技巧协同工作，以最大化价值并最大限度地减少不必要的开支。
